# bellwether-copyeditor Skill — Evaluation Report

Complete one copy of this form per test run. Save each as a separate file.

---

## Test Info

**Eval #**: 
**Date**: 
**Model**: 
**Test document**: 
**Human benchmark** *(if applicable — leave blank if no comparison set)*: 
**Prompt used**: 

---

## Edit Volume

| Metric | Skill | Human benchmark |
|---|---|---|
| Total edits made |  |  |
| Tracked changes |  |  |
| Comment-only findings |  |  |
| % of tracked changes with a comment/rationale |  | n/a |
| Human benchmark edits accepted by team *(if applicable)* |  |  |
| Skill edits matching accepted human edits *(if applicable)* |  |  |
| Skill edits unique to skill (not in human benchmark) |  |  |

**Notes**: 

---

## Tracked Change Integrity

Did the skill apply edits correctly to the docx output?

- [ ] Clean — all tracked changes applied to the correct text, no corruption
- [ ] Minor issues — a few findings misapplied or unapplied, noted below
- [ ] Significant issues — widespread misapplication or document corruption

| Issue | Location | Type (wrong text targeted / meaning altered / other) | Notes |
|---|---|---|---|
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |

**Notes**: 

---

## Edit Quality (Precision)

Were the edits the skill made correct?

- [ ] High — nearly all edits are defensible
- [ ] Mixed — some correct, some questionable
- [ ] Low — significant proportion of wrong edits

**Notes**: 

---

## False Positives

Edits the skill made that were wrong. Use the Type column to distinguish between stylistically incorrect edits (wrong rule applied, meaning changed) and mechanical failures (edit targeted wrong text or corrupted output).

| Edit | Skill's Rationale | Type (stylistic error / mechanical failure) | Notes |
|---|---|---|---|
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |

**False positive rate** (# false positives ÷ total skill edits): 

**Notes**: 

---

## Edit Recall by Category

How well did the skill cover each category of edits? If using a human benchmark, count accepted human edits only. If no benchmark, use your own judgment to estimate gaps.

| Category | Human taken edits *(if applicable)* | Skill caught (approx.) | Coverage | Notes |
|---|---|---|---|---|
| Clarity |  |  |  |  |
| AP Style |  |  |  |  |
| Wordiness |  |  |  |  |
| Formatting |  |  |  |  |
| Repetitive phrasing |  |  |  |  |
| Grammar |  |  |  |  |
| Precision |  |  |  |  |
| Word Choice |  |  |  |  |
| Parallelism |  |  |  |  |
| Bellwether Style |  |  |  |  |
| Inclusive Language |  |  |  |  |
| Factual Flag |  |  |  |  |

**Notes**: 

---

## Comment Quality

Did the skill provide useful rationale for its edits?

- [ ] Yes — comments are specific, labeled by issue type, and actionable
- [ ] Partially — some comments present but inconsistent
- [ ] No — edits made without explanation

**Notes**: 

---

## Clarifying Questions

Did the skill ask appropriate clarifying questions before proceeding?

- [ ] Asked the right question(s) — specify: _______________
- [ ] Asked unnecessary or confusing questions — specify: _______________
- [ ] Should have asked but didn't — specify: _______________
- [ ] No questions needed for this task

**Notes**:

---

## Overall

**How does the output compare to a professional copy edit?**
- [ ] Matches or exceeds human editor quality
- [ ] Solid first pass; would benefit from a human review
- [ ] Significant gaps; requires substantial human follow-up

**Anything unexpected or off?**

---

## Summary Rating

1 = Poor, 2 = Acceptable, 3 = Good

| Dimension | Rating | Notes |
|---|---|---|
| Edit precision |  |  |
| Edit recall |  |  |
| Tracked change integrity |  |  |
| Comment/rationale quality |  |  |
| AP Style coverage |  |  |
| Bellwether style coverage |  |  |
| Formatting coverage |  |  |
| Overall usefulness |  |  |
